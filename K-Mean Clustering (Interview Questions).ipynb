{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf311ba8",
   "metadata": {},
   "source": [
    "Q) What is Clustering and where to use it in real world?\n",
    "\n",
    "* Clustering a technique to grouping the data points into different groups called clusters based on their similarities. It can be used to solve real-world problems in different fields.\n",
    "\n",
    "\n",
    "Clustering is a widely used technique in various real-world applications across different domains. Here are some examples:-\n",
    "\n",
    "\n",
    "1) Customer Segmentation: Clustering can be used to group customers based on their preferences, behavior, and demographics. This information can be used to target specific customer segments with personalized marketing campaigns.\n",
    "\n",
    "\n",
    "2) Cross Selling and Upselling: Clustering can be used to group customers based on their past perforamce and then companies tries to cross sell and upsell their products to the customers which have good performace history.\n",
    "\n",
    "\n",
    "3) Recommendation Systems: It is used to create recommendation systems of e-commerce websites and OTT platforms like Netflix, Amazon Prime, etc. to provide recommendation of a product or video based on the past search of a product, movie respectively.\n",
    "\n",
    "\n",
    "4) Search Engine optimization: Clustering can be used to group content into different categories or topics, making it easier for users to find relevant information. Eg:- On new applications or website their are many kinds of news available, clustering can be used to group the news based on their category i.e sports news, crime news, weather news, etc.\n",
    "\n",
    "\n",
    "5) Resume Classification: In companies there are thousand's of people who applied for different job roles. Clustering can be used to group resumes based on the content of the resume, such as education, experience, skills, and achievements. This can help HR departments to automatically categorize and filter resumes for specific job roles, saving time and effort in the recruitment process. Once resumes are clustered, the HR department can easily target specific clusters for specific job openings. Moreover, clustering can also help to identify top candidates based on their resume content, enabling HR to focus on the most qualified candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc6667f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa33b2c1",
   "metadata": {},
   "source": [
    "Q) What are the different machine learning algorithms used for clustering ?\n",
    "\n",
    "There are several machine learning algorithms used for clustering, some of which are:\n",
    "\n",
    "1) K-means Clustering: It is one of the most popular clustering algorithms used in machine learning. It aims to partition the dataset into k clusters, where k is the number of clusters specified by the user.\n",
    "\n",
    "\n",
    "2) Hierarchical Clustering: It is a clustering algorithm that creates a hierarchy of clusters. There are two types of hierarchical clustering algorithms: Agglomerative and Divisive. \n",
    "\n",
    "\n",
    "3) Density-based Clustering: Density-based clustering algorithms, such as DBSCAN(Density-Based Spatial Clustering of Applications with Noise.) and OPTICS, group data points based on their density. Data points that are close to each other and have high density are considered to be part of the same cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a594a146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cebb3d54",
   "metadata": {},
   "source": [
    "Q) What will you do if you have labelled data and you have to perform unsupervised machine learning?\n",
    "\n",
    "* Drop the labelled feature: One approach is to drop the labelled feature from the dataset and perform unsupervised clustering on the remaining features. You can then compare the resulting clusters with the known labels to gain insights into the structure of the data. This can be useful for exploratory analysis.\n",
    "\n",
    "\n",
    "* Use the labelled feature as an input: Another approach is to include the labelled feature as an input to the clustering algorithm. In this case, the clustering algorithm will take into account both the labelled and unlabelled features to group similar data points together. This approach can be useful if the labelled feature is highly informative or if you want to use the clustering results to make predictions or build a classification model.\n",
    "\n",
    "\n",
    "Note:- This Approaches can perform in semi-supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25856583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdbf485b",
   "metadata": {},
   "source": [
    "Q)  What is K-Means Clustering and how does it work?\n",
    "\n",
    "K-Means Clustering is a popular unsupervised machine learning algorithm used for clustering data points into groups or clusters based on their similarity. The algorithm works by partitioning the dataset into K clusters, where K is the number of clusters specified by the user.\n",
    " \n",
    " \n",
    "The K-Means Clustering algorithm works as follows:\n",
    "\n",
    "\n",
    "1) Choose the number of clusters, K by the user.\n",
    "\n",
    "\n",
    "2) Initialize K centroids randomly.\n",
    "\n",
    "\n",
    "3) Measuring the euclidean distance between each data point and each centroid, then assisgn data point to the nearest centroid and form K number of clusters.\n",
    "\n",
    "\n",
    "4) Calculate the centroid of each cluster by taking the mean of data points present in a particular cluster.\n",
    "\n",
    "\n",
    "5) Then, the centroid of each cluster will move its position. Then, algorithm again measures the euclidean distance between each data point and each centroid and assign the data point to the nearest cluster.\n",
    "\n",
    "\n",
    "6) This process is iteratively repeated untill the position of centroids will fixed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa715e08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "027a0d65",
   "metadata": {},
   "source": [
    "Q) How to check or measure the quality of clusters?\n",
    "\n",
    "* The quality of the clustering is measured using the within-cluster sum of squares (WCSS), which represents the sum of the squared distance between each data point and its assigned centroid. The goal of the algorithm is to minimize the WCSS by creating clusters that are compact and well-separated. It means the WCSS of the cluster is low and the distance between clusters are high, consider as a good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59906826",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a0d4fa",
   "metadata": {},
   "source": [
    "Q) What is inter-cluster distance and intra-cluster distance? \n",
    "\n",
    "\n",
    "* Intra-cluster Distance:- Intra-cluster distance is the distance between all the data points to its centroid within the same cluster. It is a measure of how tightly packed the data points are within a cluster, and it is typically measured using Euclidean distance or some other distance metric. The smaller the intra-cluster distance, the more similar the data points are to each other, and the more compact the cluster is.\n",
    "\n",
    "\n",
    "* Inter-Cluster Distance: Inter-cluster distance, is the distance between the centroids of two different clusters. It is a measure of dissimilarity between different clusters, and it is also typically measured using Euclidean distance or some other distance metric. The larger the inter-cluster distance, the more dissimilar the data points in different clusters are to each other, and the more separated the clusters are.\n",
    "\n",
    "\n",
    "In summary:\n",
    "\n",
    "* Intra-cluster distance: distance between data points and the centroid of the same cluster, measure of how tightly packed the data points are within a cluster.\n",
    "\n",
    "\n",
    "* Inter-cluster distance: distance between the centroids of different clusters, measure of how separated different clusters are.\n",
    "\n",
    "\n",
    "* Model is consider to be good, when intra-cluster distance should be less and inter-cluster distance should be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cc65af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "168b8260",
   "metadata": {},
   "source": [
    "Q) What is the objective function of K-means clustering?\n",
    "\n",
    "* The objective function of K-means clustering is to minimize the within-cluster sum of squares (WCSS), which is the sum of the squared distances between each data point and the centroid of its assigned cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb68d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af87a476",
   "metadata": {},
   "source": [
    "Q) Does the clusters overlap?\n",
    "\n",
    "* No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f80872",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c285d4a1",
   "metadata": {},
   "source": [
    "How do you select the optimal number of clusters in K-means clustering?\n",
    "\n",
    "1) Elbow Method: The optimal number of clusters in K-means clustering can be selected using the elbow method, which involves plotting the WCSS as a function of the number of clusters and selecting the number of clusters where the rate of decrease in WCSS begins to slow down.\n",
    "\n",
    "\n",
    "2) Silhouette Analysis: It is a method for evaluating the quality of clusters. Silhouette Analysis provides a score between -1 to +1. The value closer to +1 indicates the better quality cluster. It can be used to find the optimal number of clusters by average the silhouette score of clusters. Example:- We compare the all silhouette scores for possible number of clusters and for which number of clusters silhouette score is high we consider that number of clusters as a optimal number.\n",
    "\n",
    "\n",
    "* We can calculate the average silhouette score for different values of K and choose the value of K that maximizes the score. The optimal number of clusters is that which have the highest average silhouette score. This approach is useful in cases where the elbow method is inconclusive or unclear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117c8d6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ca9518a",
   "metadata": {},
   "source": [
    "Q) What is elbow method?\n",
    "\n",
    "\n",
    "* The elbow method is used in K-means clustering to identify the optimal number of clusters to use for a given dataset.\n",
    "\n",
    "\n",
    "* The method works by plotting the within-cluster sum of squares(WCSS) on y-axis againt the number of clusters on x-axis.\n",
    "\n",
    "\n",
    "* The WCSS is the sum of the squared distances between each data point and the centroid of its assigned cluster. As the number of clusters increases, the WCSS tends to decrease because the data points are closer to their respective centroids.\n",
    "\n",
    "\n",
    "* The elbow method involves visually inspecting the plot and identifying the point of inflection or \"elbow\" where the rate of decrease in WCSS begins to slow down. The number of clusters corresponding to this point is considered to be the optimal number of clusters for the given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b27486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e766325c",
   "metadata": {},
   "source": [
    "Q) How K-Means++ works?\n",
    "\n",
    "K-Means++ is an improved version of K-Mean clustering that deals with the limitations of K-mean clustering.\n",
    "\n",
    "\n",
    "1) K-Means++ first select a randomly data point as a centroid, then it will calculate the sqaured distance between all data points to the first centroid. \n",
    "\n",
    "\n",
    "2) The data point which having the farest distance from first centroid is consider to be the next centroid.\n",
    "\n",
    "\n",
    "3) This process is iteratively repeated untill we find the k number of centroids.\n",
    "\n",
    "\n",
    "4) Further process is same as K-Means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01248414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e6a303f",
   "metadata": {},
   "source": [
    "Q) What is the problem in K-Means clustering that we use K-Means++?\n",
    "\n",
    "1) K-Means clustering initializes the centroids randomly insted of selecting a data point as a centroid, so there is a chance that it will select centroids at suboptimal positions, which results in making poor clusters.\n",
    "\n",
    "\n",
    "2) If we set k=2 or 3, there is a chance that it may only make one cluster of the whole dataset due to selecting centroids at suboptimal positions.\n",
    "\n",
    "\n",
    "3) K-Means++ selects the first centroid randomly from the data points, then calcuate the squared difference between all datapoints to first centroid and the data point which having the farest distance consider as a next centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a2793d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7b6c13c",
   "metadata": {},
   "source": [
    "Q) Difference between K-Mean Clustering and K-Mean++ Clustering?\n",
    "\n",
    "K-Mean clustering and K-Means++ clustering are both methods used for clustering data points into groups or clusters. Here are the key differences between them:\n",
    "\n",
    "\n",
    "1) Centroid initialization: In K-Mean clustering, centroids are randomly initialized, which means if the centroids are initialized at suboptimal position, which results in poor clustering result and there may be a chance that if we give k=2, it only make 1 cluster. To avoid this problem of Centroid initialization we use K-Mean++, which initialization the centroid in more better way, and results good clustering result. K-Means++ consider a data point as a centroid instead making centroid at any position whether it is a data point or not.\n",
    "\n",
    "\n",
    "2) Speed: K-Means++ is slower than K-Mean clustering because of its complex process to find centroid. But it produces better results than K-Mean Clustering.\n",
    "\n",
    "\n",
    "3) Quality of clusters: K-Means++ usually produces better quality clusters than K-Mean clustering, as it finds the centroid in much better way.\n",
    "\n",
    "\n",
    "4) Robustness: K-Means++ is more robust to noise and outliers than K-Mean clustering, as it is less sensitive to the initial position of centroids. This means that it can handle noisy or complex datasets better than K-Mean clustering.\n",
    "\n",
    "\n",
    "5) In the initialization step of K-Means++, squared distance is used to select the initial centroids. Once the initial centroids have been selected, the algorithm switches to using the Euclidean distance to measure the distance between each data point and the centroid and K-means doesn't use squared difference to initialize centroids.\n",
    "\n",
    "\n",
    "K-Means++ is an improved version of K-Mean clustering that addresses some of its limitations, such as poor centroid initialization and sensitivity to noise and outliers. However, it is slower than K-Mean clustering and requires more computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe9f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
