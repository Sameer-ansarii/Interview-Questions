{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb4f729",
   "metadata": {},
   "source": [
    "Q1) What id Decision Tree?\n",
    "\n",
    "\n",
    "* A decision tree is a supervised machine learning algorithm used in data science to build a model for making decisions. \n",
    "\n",
    "\n",
    "* It is used for both classification and regression problems but we usually use decision tree in classification problems because it performs good on classification problems rather than the regression problems.\n",
    "\n",
    "\n",
    "*  It consists of nodes representing the features, branches representing the conditions, and leaves representing the outcomes or decisions.\n",
    "\n",
    "\n",
    "* Decision Tree work by creating a tree-like structure of if-else conditions based on input variables and produce a output when it reachs the leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffb84b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38519c44",
   "metadata": {},
   "source": [
    "Q2) How does Decision Tree works?\n",
    "\n",
    "\n",
    "* The algorithm starts by selecting the root node, which is the best feature of the dataset, and then splits the root node into branches and branches into decision nodes. The decision tree tries to create every decision node as possible as homogeneous with respect to the target variable. (Homogeneous decision nodes means high degree of predictability or correlation with the target variable).\n",
    "\n",
    "\n",
    "* If any branch of root node or decision nodes produce leaf node, then the decision tree stops growing for that particular branch.\n",
    "\n",
    "\n",
    "* If a leaf node does not come out from the branch, then the decision tree finds the another decision node for that branch and split it further.\n",
    "\n",
    "\n",
    "* This process iteratively repeats until the decision tree finds the leaf node for every branch.\n",
    "\n",
    "\n",
    "* In summary, the decision tree finds the most important feature, which is the root node, and splits it according condition which is called branches. If any branch produces a leaf node then the tree stops growing for that branch. Otherwise, the decision tree will select another decision node and then split it into branches. The decision tree iteratively repeats this process until all the branches produces leaf node. After the root node, all the nodes are called decision nodes and their conditions are called branches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e23214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30c2c262",
   "metadata": {},
   "source": [
    "Q3) What are decision node / internal node?\n",
    "\n",
    "\n",
    "* After the root node every feature take by decision tree for splitting is called decision node or internal node. Every decision node connects with branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87de85dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "197e9ab4",
   "metadata": {},
   "source": [
    "Q4) What is if-else condition in Decision Tree?\n",
    "\n",
    "\n",
    "* The Decision tree splits the feature into branches which is also known as conditions.\n",
    "\n",
    "\n",
    "* If any condition met the criteria according data it will generate outcome or leaf node. \n",
    "\n",
    "\n",
    "* If the criteria is not met then the decision tree will select another decision node and split it into conditions.\n",
    "\n",
    "\n",
    "* This process is iteratively repeating untill all the conditions produces leaf node.\n",
    "\n",
    "\n",
    "* This process of finding the leaf node for all branches is known as if-else condition.\n",
    "\n",
    "\n",
    "* The branches in a decision tree are also known as conditions or if-else conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aefa8d05",
   "metadata": {},
   "source": [
    "Q5) What are advantages and disadvantages of decision trees?\n",
    "\n",
    "\n",
    "Advantages:-\n",
    "\n",
    "\n",
    "* Decision tree are easy to understand and explain, that why decision tree are preffered by both beginners and experts in machine learning.\n",
    "\n",
    "\n",
    "* Decision trees can be used for both classification and regression problems.\n",
    "\n",
    "\n",
    "* Decision trees can capture non-linear relationships between features and outcome, not like regression models.\n",
    "\n",
    "\n",
    "* Decision trees can provide feature importance ranking, which can be useful for feature selection and data exploration.\n",
    "\n",
    "\n",
    "Disadvantages:-\n",
    "\n",
    "\n",
    "* Decision trees can easily overfit the training data, meaning that decision tree may fit the training data too well resulting in poor performance on test data and new data.\n",
    "\n",
    "\n",
    "* Decision trees can be unstable because if we slightly change the data it will generate completely different tree, which is harder to understand and interpret.\n",
    "\n",
    "\n",
    "* Decision tree usually work best with classification problems rather than regression problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5047b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37700045",
   "metadata": {},
   "source": [
    "Q6) How do you handle missing values in a decision tree?\n",
    "\n",
    "There are several ways to handle with missing values in a decision tree:-\n",
    "\n",
    "\n",
    "* Remove the missng values.\n",
    "\n",
    "\n",
    "* Impute the missing values with mean, median, or most frequent value.\n",
    "\n",
    "\n",
    "* Impute the missing values with the help of domain expert.\n",
    "\n",
    "\n",
    "* Use an algorithm that is designed to handle missing values, such as the random forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5e21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9541c1f2",
   "metadata": {},
   "source": [
    "Q7) What is overfitting in decision trees, and how can you prevent it?\n",
    "\n",
    "\n",
    "* Overfitting is occuring when the decision tree model fits the training data too closely, resulting in poor performance on test and new data.\n",
    "\n",
    "\n",
    "There are several techniques to prevent overfitting in decision trees:- \n",
    "\n",
    "\n",
    "* By using Prunning Techniques like pre-prunning and post-pruning.\n",
    "\n",
    "\n",
    "* Using ensemble learning techniques like bagging, boosting and random forest.\n",
    "\n",
    "\n",
    "* By selecting the best parameters and their limit with the help of Random Search CV and Grid Search Cv which is used in both pre-pruning and post-pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f8dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "028e3ffd",
   "metadata": {},
   "source": [
    "Q8) What is pruning, and why is it important in decision trees?\n",
    "\n",
    "\n",
    "* Pruning is a technique used in decision trees to prevent overfitting by removing the branches of the features that do not helps to improving the accuracy of the algorithm on test or unseen data. Pruning can be done either before or after the tree is built. If we prune the tree before it builds it is known as pre-prunning and if we prune the tree after it builds it is known as post-prunning.\n",
    "\n",
    "\n",
    "* Pruning is important because decision trees can easily overfit the training data, resulting in poor performance on new data. By reducing the complexity of the tree, pruning can help to improve the accuracy of the model on new data and make it more interpretable and easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadf4015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae861b32",
   "metadata": {},
   "source": [
    "Q9) What is pre-prunning and post-prunning?\n",
    "\n",
    "Pre-pruning and post-pruning are two techniques used in machine learning to prevent decision trees from overfitting on training data.\n",
    "\n",
    "\n",
    "* Pre-prunning can be de done before building a decision tree by choosing the best parameters and their limit with the help of Grid Search CV and Random Search CV which are hyperparameter tunning techniques and we can also choose parameters and their limit manually. Some parameters are criterion, max_depth, min_samples_split, min_samples_leaf, max_features, splitter, etc. After selecting the parameters with their limit we build the decision tree.\n",
    "\n",
    "\n",
    "* Post-prunning can be done after building a decision tree. If the problem of overfitting occurs then we evaulate the decision tree by setting the best parameters with their limit with the help of hyperparameter tunning or manually or we can also use ccp_alpha hyperparameter and built decision tree again. In other words we remove the branches of decision nodes which can't help to improve the performance of decision tree on unseen data and we do this manually by setting the parameters with their limit or we can use feature_importance to find best feature and make model again only on those features which are important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc2859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c942214b",
   "metadata": {},
   "source": [
    "Q10) Can you explain the difference between classification and regression trees?\n",
    "\n",
    "\n",
    "Classification and regression trees are two types of decision trees used in machine learning.\n",
    "\n",
    "\n",
    "* A classification tree is used to predict the categorical output such as labels or classes of output variable with the help of input variables. For example, a classification tree could be used to predict that a person is eligible for loan or not and if bank give him loan then he will able to repay it or not?\n",
    "\n",
    "\n",
    "* A regression tree is used to predict the continuous numerial output. Eg:- price of a houses, price of products, etc. For eg:- Decision tree could be used to predict that how much a person will spend on a particular product on ecommerce website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9d5f34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccfab81a",
   "metadata": {},
   "source": [
    "Q11) Can you explain the bias-variance tradeoff in decision trees?\n",
    "\n",
    "\n",
    "The bias-variance tradeoff in decision trees refers to the balance between bias and variance of model.\n",
    "\n",
    "\n",
    "* Bias mean the inability of model to truly capture the pattern and relationship in the trainning data or we can say bias is the measurement to find how accurately a model can capture a pattern and relationship in trainning dataset.\n",
    "\n",
    "\n",
    "* Variance means how model perform on different sets of data. The performance error between datasets is called variance.\n",
    "\n",
    "\n",
    "* If the model is high biased then it will perform well on trainning but not on test and unseen data. If the model is high variance it means the performance error is high on different sets of data. It means the problem of overfitting occurs.\n",
    "\n",
    "\n",
    "* If the model has high variance and low biased, it means the model perform well on unseen data but not on trainning data. That means the problem of underfitting occurs.\n",
    " \n",
    "\n",
    "* The goal of a decision tree is to prevent from overfitting and underfitting and minimize both bias and variance in order to build a model that perform well on trainning data as well as on test and unseen data.\n",
    "\n",
    "\n",
    "* The techniques like prunning will help to reduce the variance and ensemble techniques like bagging and boosting used to reduce both bias and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9566f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f5aed1f",
   "metadata": {},
   "source": [
    "Q12) How do you deal with imbalanced data in decision trees?\n",
    "\n",
    "\n",
    "* Imbalanced data means when one category is in majority than the other category is in minority in output variable. \n",
    "\n",
    "\n",
    "* This can biased the result of decision tree towards majority class.\n",
    "\n",
    "\n",
    "* There are many techniques to deal with imbalanced data like using of resampling method such as oversampling or undersampling techniques, adjust the sample weights means give the high weightage or importance to minorty class or we can use ensemble learning techniques like bagging, boosting or random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25583d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "313f50b3",
   "metadata": {},
   "source": [
    "Q13) How do you interpret the results of a decision tree? / How can you explain the result of decision tree?\n",
    "\n",
    "\n",
    "* To interpret or explain the results of a decision tree, we can visulaize and examine the decision tree structure from the root node to the leaf nodes.\n",
    "\n",
    "\n",
    "* First we exaine the root node which is the best and important feature of the data. This feature has impact the most on the output that's why decision tree select it as root node.\n",
    "\n",
    "\n",
    "* Secondally we examine the internal nodes or decision nodes on which decision tree further split the data. Decision tree try to make every decision node as homogenous means every feature in the data can impact the output.\n",
    "\n",
    "\n",
    "* Thirdly we examine that on what conditions it will split the root node and decision nodes. These conditions are used to find the output.\n",
    "\n",
    "\n",
    "* Lastly we examine the leaf nodes which is the output given by decision tree based on the input variables. Decision tree will generate leaf nodes for all branches of decision nodes. It means it generate output for all conditions of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec5a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cc936a4",
   "metadata": {},
   "source": [
    "Q14) How do you choose the optimal tree depth?\n",
    "\n",
    "\n",
    "* The optimal depth of the tree depends on the complexity of the problem and the size of the data.\n",
    "\n",
    "\n",
    "* If the depth of the tree is too deep then it may occurs to overfitting and if the depth of tree is too shallow it may occurs to underfitting.\n",
    "\n",
    "\n",
    "* By doing cross validation on different depths of tree we find the accurate depth of the tree on which model perform well among all possible depths and cross validation can be done by hyperparameter tunning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605dfae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1f63be5",
   "metadata": {},
   "source": [
    "Q15) Decision tree work on if else condition then why we don't use rule based programming(if-else conditions in python) instead of Decision Tree? \n",
    "\n",
    "\n",
    "* Ruled based programming is not efficiently work on large set of data.\n",
    "\n",
    "\n",
    "* If there's a minor change in data will happen we need to update the program again but decision tree handle changes in data by self.\n",
    "\n",
    "\n",
    "* Decision tree learn by self and don't require human interactions but ruled-based programming requires human interaction to set conditions and rules in program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8db3b20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d1235a2",
   "metadata": {},
   "source": [
    "Q16) What is sub-tree?\n",
    "\n",
    "\n",
    "* It is small part or subset of decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78d121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24984df2",
   "metadata": {},
   "source": [
    "Q17) What is splitting point in decision tree?\n",
    "\n",
    "\n",
    "* From where the node splits the data that point is known as splitting point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07a0c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0d19f36",
   "metadata": {},
   "source": [
    "Q18) How do you decide which split criterion to use in a decision tree?\n",
    "\n",
    "\n",
    "* The split criterion depends on the type of problem and type of data we have to solve the problem. \n",
    "\n",
    "\n",
    "* There are three criteria used in decision tree: entropy, Gini impurity, and information gain. Entropy and Gini impurity measures the impurity or uncertainty of the feature based on the number of examples they have while information gain measures the importance of a feature. Information gain can be used with both entropy and Gini impurity, but we have to select between gini and entropy.\n",
    "\n",
    "\n",
    "* We should try both of them and check the results. The criteria which gives better result we choose them.\n",
    "\n",
    "\n",
    "* We should also try hyperparameter tunning to find the best criterion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fdda1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c02e9576",
   "metadata": {},
   "source": [
    "Q20) What is pure node?\n",
    "\n",
    "\n",
    "*  A pure node in a decision tree that contains only one class or label of output. In other words, all the samples in that node belong to the same class of output or we can say if decision node contains only one situation it is pure node and when it contains more than one condition it is impure.\n",
    "\n",
    "\n",
    "* Leaf Node also known as terminal node/pure node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f522bacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ade033f",
   "metadata": {},
   "source": [
    "Q21) Which is best criterion entropy or gini and which is fast to compute?\n",
    "\n",
    "\n",
    "* It is depend on the problem statement and given dataset which criteria gives the better result. On most of the datasets both gives similar results.\n",
    "\n",
    "\n",
    "* Gini is computational faster than entropy due to its simple calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8f0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30a62780",
   "metadata": {},
   "source": [
    "Q22) What is entrophy? \n",
    "\n",
    "\n",
    "* Entropy is a measure of the impurity or uncertainty of a dataset.\n",
    "\n",
    "\n",
    "* It helps to find the feature for each node and the conditions on which it splits the feature.\n",
    "\n",
    "\n",
    "* Entropy value find based on the number of observations/samples each node consists.\n",
    "\n",
    "\n",
    "* The objective of using entropy is to identify the feature and its conditions that maximize the information gain, which reduces the impurity of the dataset and improves the accuracy of the decision tree model.\n",
    "\n",
    "\n",
    "* After choosing a feature for a node it try to find the conditions in such a way that all branches or conditions produce leaf node.\n",
    "\n",
    "\n",
    "* The value of entropy ranges between 0-1. The lower entropy value indicates the better purity and high information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c32ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b1b08a4",
   "metadata": {},
   "source": [
    "Q23) How to find entropy?\n",
    "\n",
    "\n",
    "* We have 2 categories of outcome, 30 Yes and 70 No\n",
    "\n",
    "\n",
    "* Entropy = -Σ P * log2(P)\n",
    "* Entropy = -Py log2 (Py) - Pn log2 (Pn)\n",
    "* Entropy = -30/100 * log2 * (30/100) - 70/100 * log2 * (70/100)\n",
    "* Entropy = (-1.737 * 0.3) + (-0.514 * 0.7)\n",
    "* Entropy = 0.881\n",
    "\n",
    "\n",
    "* Or we can say -Submittion of sigma, probability of class1 * log2 * probability of class1 - probability of class2 * log2 * probability of class2\n",
    "\n",
    "\n",
    "* The probability of Yes is 30/100 = 0.3\n",
    "* The probability of No is 70/100 = 0.7\n",
    "* The log base 2 of 0.3 is approximately -1.737\n",
    "* The log base 2 of 0.7 is approximately -0.514\n",
    "* Multiply -1.737 by 0.3 and -0.514 by 0.7\n",
    "* Add up the two results: (-1.737 * 0.3) + (-0.514 * 0.7) = 0.881\n",
    "* Multiply the result by -1: 0.881 * -1 = -0.881"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cc1486",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39e660f5",
   "metadata": {},
   "source": [
    "Q24) What is gini?\n",
    "\n",
    "\n",
    "* Gini is another measure of the impurity or uncertainty of a dataset, similar to entropy.\n",
    "\n",
    "\n",
    "* Same as entropy gini is used to find the feature and the condition on which the feature splits and try to produce pure node for every branch.\n",
    "\n",
    "\n",
    "* Same as entropy it's objective is to minimize the impurity of features in data and maximum the information gain for producing maximum pure nodes as possible.\n",
    "\n",
    "\n",
    "* Same as entropy the value of gini ranges between 0-1. The lower gini value indicates the better purity and high information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2128e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "297861b3",
   "metadata": {},
   "source": [
    "Q25) How to find Gini value?\n",
    "\n",
    "\n",
    "* We have 2 categories of outcome, 30 Yes and 70 No.\n",
    "\n",
    "\n",
    "* Gini = 1-(Py² + Pn²)\n",
    "\n",
    "\n",
    "* or we can say 1 minus square of probability of class1 + square of probability of class2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894cf18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee501d5",
   "metadata": {},
   "source": [
    "Q26) Uses of Entropy and gini?\n",
    "\n",
    "\n",
    "* They are used to measure the impurity / randomness / uncertainity of data.\n",
    "\n",
    "\n",
    "* They used to find feature for nodes and their conditions on which it can split.\n",
    "\n",
    "\n",
    "* They try to find the condition on such a way that all branches of conditions produces the leaf node as possible.\n",
    "\n",
    "\n",
    "* They are the measure to evaulate the quality of split or branch, whether the split is pure or not.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bbebde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06bbe49",
   "metadata": {},
   "source": [
    "Q27) What is the difference between Gini and entropy?\n",
    "\n",
    "They both are same but there are some differences:-\n",
    "    \n",
    "\n",
    "* They both measures the impurity and uncertainty in data but the difference is in their formulas.\n",
    "\n",
    "\n",
    "* The formula of entropy if -Σ Pi * log2(Pi). Which is -probability of class1 * log2 * probability of class1 - probability of class2 * log2 * probability of class2.\n",
    "    \n",
    "\n",
    "* The formula of gini is 1-(Py²+Pn²) which is 1-square of probabilty of class1 + square of probabilty of class2.\n",
    "\n",
    "\n",
    "* Gini is computational fast than entropy due to its smaller formula than entropy.\n",
    "\n",
    "\n",
    "* Gini index work better with binary classification and entropy is work better with both binary as well as multi-class classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3245a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b40dee9",
   "metadata": {},
   "source": [
    "Q28) What is information gain?\n",
    "\n",
    "\n",
    "* Information gain is a measure used to select the best feature which explain the target variable the most.\n",
    "\n",
    "\n",
    "* Information gain is calcualted for each feature and for every node.\n",
    "\n",
    "\n",
    "* The goal of information gain is to find the feature in such a way that when we split the feature the value of entropy or gini can be minimize.\n",
    "\n",
    "\n",
    "* The feature with the highest information gain is selected as the best feature for splitting the data, because it minimize the entropy or gini value after splitting.\n",
    "\n",
    "\n",
    "* The higher information gain value indicates the usefullness of feature and result in lower entropy/gini value after the splitting.\n",
    "\n",
    "\n",
    "* This process is repeated recursively for each node, until a stopping criterion is met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82134924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5139fa9",
   "metadata": {},
   "source": [
    "Q29) How to calculate information gain?\n",
    "\n",
    "\n",
    "* The formula for information gain based on entropy is: Information Gain = Entropy(parent) - [weighted average * Entropy(children)]\n",
    "\n",
    "\n",
    "* The formula for information gain based on Gini impurity is: Information Gain = Gini(parent) - [weighted average * Gini(children)]\n",
    "\n",
    "\n",
    "Where:-\n",
    "\n",
    "\n",
    "* Entropy(parent) = Entropy value of particular variable. \n",
    "* Weightage average = sum of Category1 of independent variable / Total number of observations\n",
    "* Entropy(children) = Entropy of category of independent variable.\n",
    "\n",
    "We have independent variable climate which consists of (rain, hot, cold) and dependent variable child play (Yes/No).\n",
    "\n",
    "\n",
    "* Information gain = Entropy(child play) - [sum of rain / total no. of observations * Entropy(rain) + sum of hot / total no. of observations * Entropy(hot) + sum of cold / total no. of observations * Entropy(cold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6f4947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dd3640b",
   "metadata": {},
   "source": [
    "Q30) Difference between Regression models and decision tree?\n",
    "\n",
    "Regression models and decision trees are both techniques used in supervised machine learning for predicting the value of a dependent variable based on one or more independent variables. However, there are some key differences between these two techniques:-\n",
    "\n",
    "\n",
    "* Regression models works on mathematical equations to predict the output where as decision tree is a hierarchical structure of nodes which works by passing the conditions according independent variables to predict output.\n",
    "\n",
    "\n",
    "* Decision tree captures non-linear relationship of variables more easily where as regression models can not deal with it directly we need use other functions like polynomial term or other non-linear transformations of the input variables.\n",
    "\n",
    "\n",
    "* The problem of overfitting more occurs in decision tree due to its complex hierarchical structure where as regression models are less prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f0868f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99b556b4",
   "metadata": {},
   "source": [
    "Q31) What is the impact of outliers in decision tree?\n",
    "\n",
    "\n",
    "* Outliers can have a significant impact on decision trees.\n",
    "\n",
    "\n",
    "* Decision trees are constructed by recursively splitting the data into subsets based on the values of the input variables, and each split is chosen to minimize the value of entropy or gini. If outliers present in the data it will increase the value of impurity measures and impurity as well.\n",
    "\n",
    "\n",
    "There are several approaches to deal with outliers:-\n",
    "\n",
    "\n",
    "* Remove outliers.\n",
    "\n",
    "\n",
    "* Use ensemble methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60f7097",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da68c3e5",
   "metadata": {},
   "source": [
    "Q32) How decision tree regressor works?\n",
    "\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d9d01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dd677",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
