{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10bf8ebb",
   "metadata": {},
   "source": [
    "# Important Points to be Note:-\n",
    "    \n",
    "1) Apply single scaler to scaler all numeric values. All values should be scaled in a specific range no matter whether they are continuous numeric values or discrete or binary numeric values.\n",
    "\n",
    "\n",
    "2) After converting all categorical variables to numerical, check the distribution of all variables whether they are continuous or discrete, if the skewness is present in the variables, apply transformation techniques to normally distributed the variables. Check the skewness in only independent variables. If the skewness is present in the variables the majority values biased the model, because our model learn on the majority values. So, in future if values of independent variable are not same as majority class in which the model is trainned, it will not provide better results. \n",
    "\n",
    "\n",
    "3) Check outliers before filling null values treat them if you don't want data analysis otherwise treat outliers before converting categorical data into numerical. It is also depend on the strategy of model building.\n",
    "\n",
    "\n",
    "4) If two independent variables are highly correlated than check the correlation of each variable with target variabe and which variable has the lowest correlation with target variable, drop it.\n",
    "\n",
    "\n",
    "5) While perfroming EDA write inplace=True in your codes, otherwise the thing you want to perfrom, it will done only on temporary basis or varible of EDA not on original dataset.\n",
    "\n",
    "\n",
    "6) It is important to use the same scaling method for both the training and test sets. This will ensure that the data is scaled consistently.\n",
    "\n",
    "\n",
    "7) We should not scale the target variable (the variable that we are trying to predict). This is because the target variable is what we are trying to learn from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db40deca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba204497",
   "metadata": {},
   "source": [
    "### Q) When to use get dummies methods, label encoder and target mean encoding?\n",
    "\n",
    "Using a specific encoding technique will depends on the nature of data.\n",
    "\n",
    "* When we have nomial categorical variable, it is recommended to use one hot encoding or get_dummies method.\n",
    "\n",
    "\n",
    "* When we have ordinal categorical variable, it is recommended to use Label encoder or df.replace() method.\n",
    "\n",
    "\n",
    "* When we have multiple unique categories in the variable, such as location. It is recoomended to use target mean encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49872fdb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e3dd3a9b",
   "metadata": {},
   "source": [
    "### Q) Can we apply same Encoding to encode all variable?\n",
    "\n",
    "* No, it is not recommended to use same encosing technique for all variables. We can use multiple encoding techniques in a dateset it depends on the nature of variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2723a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27aa09d1",
   "metadata": {},
   "source": [
    "### Q) Can we apply same Scaling Method for all variables?\n",
    "\n",
    "* Yes, it is recommended to use same scaler because we want the range of all variables to be same. If the variable is already is the same range so we can not scale them. Eg: we have any binary variable so we can not scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df109f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a19c5c32",
   "metadata": {},
   "source": [
    "### Q) What is disadvantage of get_dummies method or One hot encoding?\n",
    "By using get_dummies or One Hot Encoding method the number of variables will increase. Eg: we have 4 categories in a column so it will make 4 column, 1 column for each category, and we genrally remove any 1 column so, we have total 3 columns. Due to more number of columns the chances of overfitting will increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2805ab6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b1ee31",
   "metadata": {},
   "source": [
    "### Q) What the steps involve in a Machine Learning Project in ascending order?\n",
    "\n",
    "Steps to be involved in Machine Learning Project:-\n",
    "\n",
    "1) Understand the problem Statement.\n",
    "\n",
    "\n",
    "2) Collect the data from various sources, such as databases, CSV files, APIs, or web scraping.\n",
    "\n",
    "\n",
    "3) Understand the data and their each variable very carefully.\n",
    "\n",
    "\n",
    "4) Import the data and check how dataset looks like, it shapes, data types of columns, missing values, duplicate values, correlation between variables, statistics summary of dataset(df.desribe)\n",
    "\n",
    "\n",
    "5) Data Cleansing or Data Cleaning:- Removing Duplicates, Removing irrelevant features, Correcting typing errors or spelling mistakes, create columns if data is mixed in one variable.\n",
    "\n",
    "\n",
    "6) Data Visualization:- Univariate Analysis, Bivariate Analysis, Multivariate Analysis of variables. Note the insights, caught in data visualization.\n",
    "\n",
    "\n",
    "7) Filling the missing values and treating outliers.\n",
    "\n",
    "\n",
    "8) Feature Engineering:- Creating new features if needed, convert Categorical variables into numerical by using encoding techniques. Selection of encoding technique depends on nature of data.\n",
    "\n",
    "\n",
    "9) Check the skewness of data for all variables, if skewness is present transform those variables.\n",
    "\n",
    "\n",
    "10) Feature Selection:- Select and drop features by correlation-matrix.\n",
    "\n",
    "\n",
    "11) Split the data train and test: It is recommended to scale the data after train_test_split to prevent data leakage, improve model performance, and ensure consistency.\n",
    "\n",
    "\n",
    "12) Scale the data using same scale for every variable.\n",
    "\n",
    "\n",
    "13) Feature Selection:- Make a base model and perform RFE and VIF to select features.\n",
    "\n",
    "\n",
    "14) In case of imbalanced dataset use resampling techniques and build base models,  then compare results, which technique gives you the better results according problem statement go with that technique.\n",
    "\n",
    "\n",
    "15) Make multiple models of various algorithms and compare their results according problem statement. i.e if you need a model with high precision value go with that  model which gives that.\n",
    "\n",
    "\n",
    "16) In case of high-dimensional dataset apply PCA and build multiple base models of various algorithms and compare it's result with  algorithms with no pca. If model gives better   accuracy with pca go with pca otherwise choose other simple algorithms.\n",
    "\n",
    "\n",
    "17) After selecting the correct model hypertune it for more accuracy, or any other metric according problem statement.\n",
    "\n",
    "\n",
    "18) Check Cross-Validation score of model.\n",
    "\n",
    "\n",
    "19) Check the assumptions of algorithm used to build final model if needed\", is important for some algorithms, such as linear regression and logistic regression, but not for others, such as decision trees or random forest.\n",
    "\n",
    "\n",
    "20) Evaluate the model on test or unseen data.\n",
    "\n",
    "\n",
    "21) Write a linear equation of variables used to build the model in case where you select Linear Regression as a final model and in case of linear regression and logistic regression write the coefficients in sorted order. This will helps to understand that which variable impacts the most on output.\n",
    "\n",
    "\n",
    "22) Write a summary of variables that how each variable impact the output in short and detailed manner which clients understand\", is actually called feature importance analysis and is part of model interpretation.\n",
    "\n",
    "\n",
    "23) Write your opinions to deal with problem statement.\n",
    "\n",
    "\n",
    "24) Model Deployment\n",
    "\n",
    "\n",
    "25) Model monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35965a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a3145f6e",
   "metadata": {},
   "source": [
    "### Q) What is MDD?\n",
    "\n",
    "MDD stands for Model Development Document.\n",
    "\n",
    "A model development document is a detailed report that documents the process of developing a machine learning model. It provides a comprehensive overview of the model development process, from data collection and preprocessing to model selection and evaluation.\n",
    "\n",
    "Here are some sections that may be included in a model development document:\n",
    "\n",
    "1) Introduction: This section provides an overview of the project and the problem that the model is intended to solve.\n",
    "\n",
    "\n",
    "2) Data Collection and Preprocessing: This section outlines the data collection process and describes how the data was cleaned, preprocessed, and transformed for use in the model. It may also describe any feature engineering techniques that were used to create new features.\n",
    "\n",
    "\n",
    "3) Exploratory Data Analysis (EDA): This section describes the process of exploring the data to gain insights and identify any patterns or relationships between variables. It may include visualizations of the data and statistical analyses.\n",
    "\n",
    "\n",
    "4) Model Selection: This section outlines the process of selecting the appropriate machine learning model for the problem at hand. It may include a comparison of different algorithms, as well as a discussion of the advantages and disadvantages of each.\n",
    "\n",
    "\n",
    "5) Model Training: This section describes the process of training the model using the data. It may include a description of the hyperparameters used, as well as any cross-validation or regularization techniques employed.\n",
    "\n",
    "\n",
    "6) Model Evaluation: This section outlines how the performance of the model was evaluated. It may include metrics such as accuracy, precision, recall, and F1 score, as well as visualizations of the results.\n",
    "\n",
    "\n",
    "7) Results and Discussion: This section summarizes the results of the model and discusses its strengths and limitations. It may also include recommendations for future work and improvements to the model.\n",
    "\n",
    "\n",
    "8) Conclusion: This section provides a brief summary of the project and the model developed, as well as any final thoughts or recommendations.\n",
    "\n",
    "Overall, a model development document serves as a record of the model development process and can be used to communicate the results of the project to stakeholders. It can also be used to reproduce the results in the future, or to build upon the work done to develop the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c53ca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
